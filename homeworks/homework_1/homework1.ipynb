{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldZKvx0wK-pU"
      },
      "source": [
        "# Homework 1: NLP Basics and NLP Pipelines (7 + 1 points)\r\n",
        "\r\n",
        "**Welcome to homework 1!** \r\n",
        "\r\n",
        "The homework contains several tasks. You can find the amount of points you get for the correct solution in the task header. Maximum amount of points for each homework is 7 + 1 (bonus exercise). \r\n",
        "The **grading** for each task is the following: \r\n",
        "* correct answer - **full points** \r\n",
        "* insufficient solution or solution resulting in the incorrect output - **half points**\r\n",
        "* no answer or completely wrong solution - **no points**\r\n",
        "\r\n",
        "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\r\n",
        "\r\n",
        "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\r\n",
        "\r\n",
        "When writing code, make it readable. Choose appropriate names for your variables (a = 'cat' - not good, word = 'cat' - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\r\n",
        "\r\n",
        "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Before sending your solution, do the Kernel -> Restart & Run All to ensure that all your code works.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxjgwPjeMlOg"
      },
      "source": [
        "!pip install stanza \r\n",
        "stanza.download(lang='en') # download appropriate language model for your chosen language"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBVo1tJOK7Ft"
      },
      "source": [
        "import nltk\r\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\r\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords, wordnet\r\n",
        "\r\n",
        "import spacy\r\n",
        "import stanza\r\n",
        "\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import re\r\n",
        "from collections import defaultdict, Counter\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoO93eEzL3Dv"
      },
      "source": [
        "## Task 1: Find the data (0.5 points)\r\n",
        "\r\n",
        "Find large enough text data in English or any other language supported by spaCy and Stanza. If the resources for your language are very limited, you may use English or other language of your preference. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkFhvibAMHUc"
      },
      "source": [
        "**What is the language of your data?**\r\n",
        "\r\n",
        "Answer: TODO \r\n",
        "\r\n",
        "**Where did you get the text data?**\r\n",
        "\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**What kind of texts is it? (books,magazines, news articles, etc.)**\r\n",
        "\r\n",
        "Answer: TODO \r\n",
        "\r\n",
        "**What style(s) of text does your data have? (user commetaries, scientific, neutral, etc)**\r\n",
        "\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**Was it easy to download the data? If no, describe what difficulties you had and how you resolved them.**\r\n",
        "\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExUcWs0BM44Y"
      },
      "source": [
        "## Task 2: Tokenize and count statistics (1 points)\r\n",
        "\r\n",
        "Using either NLTK or Spacy tools, tokenize your text data you found in the previous exercise. \r\n",
        "\r\n",
        "P.S. If you are using Spacy, don't forget to load an appropriate module for it. \r\n",
        "\r\n",
        "**Compute and output the following:**\r\n",
        "* number of sentences\r\n",
        "* number of tokens\r\n",
        "* number of unique tokens (or types)\r\n",
        "* average length of a sentence \r\n",
        "* average length of a token\r\n",
        "* sentence length (tokens in a sentence) histogram (you can use matplotlib.pyplot for that) \r\n",
        "* token length (characters in a token) histogram (you can use matplotlib.pyplot for that) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-3e0X3EM2zC"
      },
      "source": [
        "# Replace the path with the name of your data file\r\n",
        "data_path = \"path_to_your_text_data.txt\"\r\n",
        "\r\n",
        "data = open(data_path, encoding='utf-8').read()\r\n",
        "\r\n",
        "# Split the data into sentences and tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XclveJg1L1jT"
      },
      "source": [
        "\r\n",
        "num_sentences = ...\r\n",
        "num_tokens = ...\r\n",
        "num_unique_tokens = ...\r\n",
        "avg_sentence_len = ...\r\n",
        "avg_token_len = ...\r\n",
        "\r\n",
        "print(\"Number of sentences:\", num_sentences)\r\n",
        "print(\"Number of tokens:\", num_tokens)\r\n",
        "print(\"Number of unique tokens (or types):\", num_unique_tokens)\r\n",
        "print(\"Average sentence length:\", avg_sentence_len)\r\n",
        "print(\"Average token length:\", avg_token_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8zafNOuN4PC"
      },
      "source": [
        "sentence_lengths = ... \r\n",
        "\r\n",
        "# draw the histogram \r\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZbqdQxe-5ks"
      },
      "source": [
        "token_lengths = ...\r\n",
        "\r\n",
        "# draw the histogram\r\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlrAicfJOC6S"
      },
      "source": [
        "## Task 3: Bype pair encoding (BPE) tokenization (1 point) \r\n",
        "\r\n",
        "### Task 3.1 (0.25 points)\r\n",
        "\r\n",
        "Byte pair encoding (BPE) [link text](https://en.wikipedia.org/wiki/Byte_pair_encoding) is a simple algorithm of data compression. It looks for the most frequent pair of bytes in the data and replaces it with a new byte which is not seen in the data.\r\n",
        "\r\n",
        "Recently, this idea became used in the [tokenization](https://www.aclweb.org/anthology/P16-1162.pdf). Let's say that we want to train a network that captures the meaning of words. We can have in out data the following words: low, lower, lowest. If we tokenize the text in a simple way by splitting the words as a whole, the model will probably learn the relation between low, lower, lowest. Now, imagine that we get some new text that the model didn't see during training and it has the words small, smaller, smallest and in the training data we had only the word small. Since the model didn't see smaller and smallest during the training, it will most likely fail to capture the relation.\r\n",
        "\r\n",
        "One of the ways to solve this is BPE tokenization. It learns the most frequent sequences and can split an unknown word into **subwords**. In our case, it can split smaller into ['small', 'er'] since we had small in the training data and probably many other words ending with -er. Now. instead of one unknown word, the model have two known subwords from which it can take the information.\r\n",
        "\r\n",
        "The code below builds the subwords from the text data. For the purpose of time saving, we set the number of merges to 1000.\r\n",
        "\r\n",
        "**Study the code below and answer the questions after it.**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jCjzoj1OPSp"
      },
      "source": [
        "def get_vocab(filename):\r\n",
        "    \"\"\"Gets the text from a file and splits it with spaces.\"\"\"\r\n",
        "    \r\n",
        "    vocab = Counter()\r\n",
        "    with open(filename, encoding='utf-8') as f:\r\n",
        "        for line in f:\r\n",
        "            words = line.strip().split()\r\n",
        "            for word in words:\r\n",
        "                vocab[' '.join(list(word)) + ' </w>'] += 1\r\n",
        "    return vocab\r\n",
        "\r\n",
        "def get_stats(vocab):\r\n",
        "    \"\"\"Computes the frequencies for each pair of characters in the vocab.\"\"\"\r\n",
        "\r\n",
        "    pairs = Counter()\r\n",
        "    for word, freq in vocab.items():\r\n",
        "        symbols = word.split()\r\n",
        "        for i in range(len(symbols)-1):\r\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\r\n",
        "    return pairs\r\n",
        "\r\n",
        "def merge_vocab(pair, in_vocab):\r\n",
        "    \"\"\"Merges the most frequent pair.\r\n",
        "\r\n",
        "    Arguments:\r\n",
        "    pair -- the most frequent word pair (tuple(str, str))\r\n",
        "    in_vocab -- vocabulary with frequencies (dict)\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    out_vocab = {}\r\n",
        "    bigram = re.escape(' '.join(pair))\r\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\r\n",
        "    for word in in_vocab:\r\n",
        "        out_word = p.sub(''.join(pair), word)\r\n",
        "        out_vocab[out_word] = in_vocab[word]\r\n",
        "    return out_vocab\r\n",
        "\r\n",
        "def get_tokens_from_vocab(vocab):\r\n",
        "    tokens_frequencies = Counter()\r\n",
        "    vocab_tokenization = {}\r\n",
        "    for word, freq in vocab.items():\r\n",
        "        word_tokens = word.split()\r\n",
        "        for token in word_tokens:\r\n",
        "            tokens_frequencies[token] += freq\r\n",
        "        vocab_tokenization[''.join(word_tokens)] = word_tokens\r\n",
        "    return tokens_frequencies, vocab_tokenization\r\n",
        "\r\n",
        "def measure_token_length(token):\r\n",
        "    if token[-4:] == '</w>':\r\n",
        "        return len(token[:-4]) + 1\r\n",
        "    else:\r\n",
        "        return len(token)\r\n",
        "\r\n",
        "vocab = get_vocab(data_path)\r\n",
        "\r\n",
        "print('==========')\r\n",
        "print('Tokens Before BPE')\r\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\r\n",
        "print('All tokens: {}'.format(tokens_frequencies.keys()))\r\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\r\n",
        "print('==========')\r\n",
        "\r\n",
        "num_merges = 1000\r\n",
        "for i in tqdm(range(num_merges)):\r\n",
        "    pairs = get_stats(vocab)\r\n",
        "    if not pairs:\r\n",
        "        break\r\n",
        "    best = max(pairs, key=pairs.get)\r\n",
        "    vocab = merge_vocab(best, vocab)\r\n",
        "\r\n",
        "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\r\n",
        "\r\n",
        "print('All tokens: {}'.format(tokens_frequencies.keys()))\r\n",
        "print('Number of tokens: {}'.format(len(tokens_frequencies.keys())))\r\n",
        "print('==========')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8EJzkCfOTGv"
      },
      "source": [
        "Answer the following questions: \r\n",
        "\r\n",
        "**Study the subwords from your data. Do you see any subwords that make sense from the linguistic point of view? (e.g suffixes, prefixes, common roots etc.). Provide examples.**\r\n",
        "\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**What will happen if you increase the number of merges?**\r\n",
        "\r\n",
        "Answer: TODO "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-eeRrZIOqUx"
      },
      "source": [
        "### Task 3.2 (0.75 points) \r\n",
        "Now, you are going to implement the function that splits the unknown word into subwords using the vocab that we built above.\r\n",
        "\r\n",
        "One way to do it is the following:\r\n",
        "\r\n",
        "1. Sort our vocab by the length in the descending order.\r\n",
        "2. Find the boundaries of the \"window\" that is going to search if a candidate word has a corresponding subword in the vocab. In the beginning, the starting index is 0, since we start to scan the word from the first characher. The end index is the length of the longest subword in the vocab or the length of the word if it is smaller.\r\n",
        "3. In a while loop, start looking at the possible subwords. If the subword you are looking at is in the vocab, append it to the result. Now, your new starting index is your previous end index. Your new end index is your new start index plus the length of the longest subword in the vocab or the length of the word if it is smaller than the resulting sum. If the subword is not in the vocab, we reduce the end index by one thus narrowing our search window. Finally, is the length of our window is equal to one, we put an unknown subword in the result and update our window as above.\r\n",
        "4. End the loop when we reach the end of the word.\r\n",
        "\r\n",
        "After you finish with the function, test the tokenizer on a very common word and on a very unusual word (you can even try to invent a word yourself).\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVDRAD-ZO_pa"
      },
      "source": [
        "# Sorting the subwords by the length in the descending order\r\n",
        "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (measure_token_length(item[0]), item[1]), reverse=True)\r\n",
        "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\r\n",
        "\r\n",
        "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes the word into subword using learned BPE vocab\r\n",
        "    \r\n",
        "    Arguments:\r\n",
        "    string -- a word to tokenize. Must end with </w>\r\n",
        "    sorted_tokens -- sorted vocab by frequency in descending order\r\n",
        "    unknown_token -- a token to replace the words not found in the vocab\r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    if string == '':\r\n",
        "        return []\r\n",
        "    if sorted_tokens == []:\r\n",
        "        return [unknown_token]\r\n",
        "\r\n",
        "    # We are going to store our subwords here\r\n",
        "    string_tokens = []\r\n",
        "    \r\n",
        "    # Find the maximum length of the ngram in vocab\r\n",
        "    ngram_max_len = ...\r\n",
        "    # End index is the maximum length of the ngram or the length of the string if it's smaller\r\n",
        "    end_idx = ...\r\n",
        "    # Starting index is 0 in the beginning\r\n",
        "    start_idx = 0\r\n",
        "    \r\n",
        "    while start_idx < len(string):\r\n",
        "        subword = string[start_idx:end_idx]\r\n",
        "        if subword in sorted_tokens:\r\n",
        "            ...\r\n",
        "        elif len(subword) == 1:\r\n",
        "            ...\r\n",
        "        else:\r\n",
        "            ...\r\n",
        "            \r\n",
        "    return string_tokens\r\n",
        "\r\n",
        "# The word should end with \"</w>\". For example, \"cat</w>\".\r\n",
        "word_known = '...</w>'\r\n",
        "word_unknown = '...</w>'\r\n",
        "\r\n",
        "print('Tokenizing word: {}...'.format(word_known))\r\n",
        "if word_known in vocab_tokenization:\r\n",
        "    print(vocab_tokenization[word_known])\r\n",
        "else:\r\n",
        "    print(tokenize_word(string=word_known, sorted_tokens=sorted_tokens, unknown_token='</u>'))\r\n",
        "    \r\n",
        "\r\n",
        "print('Tokenizing word: {}...'.format(word_unknown))\r\n",
        "if word_unknown in vocab_tokenization:\r\n",
        "    print(vocab_tokenization[word_unknown])\r\n",
        "else:\r\n",
        "    print(tokenize_word(string=word_unknown, sorted_tokens=sorted_tokens, unknown_token='</u>'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efwxuk66PFu3"
      },
      "source": [
        "## Task 4: Lemmatization and normalization (1 point) \r\n",
        "\r\n",
        "### Task 4.1 (0.5 points) \r\n",
        "\r\n",
        "Using either NTLK or Spacy, lemmatize your data. Make a copy of your data but this time transform all the tokens and lemmas into the lowercase.\r\n",
        "\r\n",
        "Provide the following statistics:\r\n",
        "\r\n",
        "* Number of unique lemmas (original case)\r\n",
        "* Number of unique lemmas (lower case)\r\n",
        "* Number of unique tokens (original case)\r\n",
        "* Number of unique tokens (lower case)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V_7fYSsPQEe"
      },
      "source": [
        "# Lemmatize your data\r\n",
        "...\r\n",
        "\r\n",
        "\r\n",
        "# Make a copy of your tokens but in lowercase\r\n",
        "...\r\n",
        "\r\n",
        "\r\n",
        "# Count statistics (no need to calculate the number of unique tokens in original case since we did it in Task 2)\r\n",
        "num_unique_lemmas = ...\r\n",
        "num_unique_lemmas_lower = ...\r\n",
        "num_unique_tokens_lower = ...\r\n",
        "\r\n",
        "# Print out the numbers\r\n",
        "print(\"Number of unique lemmas (original case):\", num_unique_lemmas)\r\n",
        "print(\"Number of unique lemmas (lower case):\", num_unique_lemmas_lower)\r\n",
        "print(\"Number of unique tokens (original case):\", num_unique_tokens)\r\n",
        "print(\"Number of unique tokens (lower case):\", num_unique_tokens_lower)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjFUP4SqPUup"
      },
      "source": [
        "### Task 4.2 (0.5 points)\r\n",
        "\r\n",
        "Look at the numbers you got. \r\n",
        "\r\n",
        "**Imagine that you want to use your data to train a network that captures the meaning of the words. Do you want to use tokens or lemmas? Original or lowercase? Explain your choices.**\r\n",
        "\r\n",
        "Answer: TODO \r\n",
        "\r\n",
        "**Imagine that you want to use your data to train a system that detects named entities, i.e. names of people, places, companies etc. Do you want to use tokens or lemmas? Original or lowercase? Explain your choice.**\r\n",
        "\r\n",
        "Answer: TODO\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp82pLGsPsBa"
      },
      "source": [
        "## Task 5: Different Pipelines (0.5 points) \r\n",
        "\r\n",
        "In the next tasks you need to process your data from task 1 with two different pipelines. Use Stanza and spiCy for that. \r\n",
        "\r\n",
        "**What components do the pipelines have?**\r\n",
        "\r\n",
        "Answer: TODO \r\n",
        "\r\n",
        "**What languages do the pipelines support?**\r\n",
        "\r\n",
        "Answer: TODO "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htTy9qWSQBe0"
      },
      "source": [
        "## Task 6: Process your text (2 points) \r\n",
        "\r\n",
        "### Task 6.1 (1.5 point) \r\n",
        "\r\n",
        "Process the text data from the first task with two different pipelines. Use Stanza and spiCy for that. \r\n",
        "\r\n",
        "Select one sentence from the processed document and print out all the results (tokens, pos-tags, lemmas, depparse, etc.) from both pipelines. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2vs3mXAQOk5"
      },
      "source": [
        "# Process the text \r\n",
        "# Pipeline 1\r\n",
        "...\r\n",
        "\r\n",
        "# Pipeline 2\r\n",
        "...\r\n",
        "\r\n",
        "# Print out the results from both pipelines\r\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHPPCgMaQTux"
      },
      "source": [
        "### Task 6.2 (0.5 points)\r\n",
        "\r\n",
        "**Look at the output from both pipelines. Are the results correct and do the pipelines have the same output? If no, provide the examples of the mistakes and differences.** \r\n",
        "\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**What is the difference between a POS tag and morphological tag?**\r\n",
        "\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**What is the difference between tagging and parsing?**\r\n",
        "\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**Analyze the dependency parsing result from both pipelines. Does the results make sense? Briefly describe the meaning behind the relations.**\r\n",
        "\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**Is one pipeline better than the other based on the output of one sentence?**\r\n",
        "\r\n",
        "Answer: TODO "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs1VDYjwQgqZ"
      },
      "source": [
        "## Task 7: Statistics (1 point)\r\n",
        "\r\n",
        "In your processed output (choose output from only one of the pipelines), compute and print out (in a human readable format) the following statistics: \r\n",
        "* POS tag frequency for each tag (in descending order) \r\n",
        "\r\n",
        "* 50 most frequent lemmas \r\n",
        "* 10 least frequent lemmas "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsgA9FaXQgFx"
      },
      "source": [
        "# Compute and print out POS tag frequency \r\n",
        "...\r\n",
        "\r\n",
        "\r\n",
        "# Compute and print out 50 most frequent lemmas \r\n",
        "...\r\n",
        "\r\n",
        "# Compute and print out 10 least frequent lemmas \r\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTXDnViOQ8pp"
      },
      "source": [
        "## Bonus Task: WordCloud (1 point) \r\n",
        "\r\n",
        "Wordcloud gives us a visual representation of the most common words in the data. Visualisation is key to understanding whether we are on the right track with preprocessing, it allows us to verify if we need more preprocessing before further analysing the text data. \r\n",
        "\r\n",
        "**Your task is to create three wordclouds. One wordcloud should be created before any preprocessing is done to the text data and other two is created from the preprocessed text data. \r\n",
        "Do suitable preprocessing for tasks described in 4.2. This means:**\r\n",
        "1. preprocess data, so we could train a neural network that will capture the meaning of words. \r\n",
        "2. preprocess data, so we could train a system that detects named entities.\r\n",
        "\r\n",
        "\r\n",
        "Python has a massive number of open libraries for drawing wordclouds. You can use Andreas Mueller's [wordcloud](http://amueller.github.io/word_cloud/) library to do that.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1sfiWnxZ7G9"
      },
      "source": [
        "# create the first wordcloud from raw text data\r\n",
        "...\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSi7D4GvY1JV"
      },
      "source": [
        "# Preprocess the data for task 1\r\n",
        "...\r\n",
        "\r\n",
        "# create the second wordcloud\r\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxcn5QY--AeM"
      },
      "source": [
        "# Preprocess the data for task 2\r\n",
        "...\r\n",
        "\r\n",
        "# create the third wordcloud\r\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mkTT_veaCVv"
      },
      "source": [
        "**What are the differences between these wordclouds (provide examples)? Can you say from the wordclouds if the preprocessing was enough for the tasks?**\r\n",
        "\r\n",
        "Answer: TODO"
      ]
    }
  ]
}