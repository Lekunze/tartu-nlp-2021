{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bW0AOiASyvU"
      },
      "source": [
        "# Lab 2: NLP Pipelines\r\n",
        "\r\n",
        "In this Lab, we are going to look into NLP pipelines and their role in text processing. \r\n",
        "\r\n",
        "### Pipeline\r\n",
        "A pipeline is a set of processors combined together to form a chain. A user puts their input from one end of the pipeline and gets the desired output from the other end. If you are familiar with Linux command line then you might have used the pipe command, where output of the first command becomes the input to the next command. For example: \r\n",
        "\r\n",
        "$ less text.txt | grep winter\r\n",
        "\r\n",
        "Here we first look into the text file, so the output is the whole file content and then we pipe this output into the grep command, which uses this text file to search for the word winter. We can again pipe this output into another command and so on. This is what happens with NLP pipeline as well. We first start with one processing task (generally tokenization or segmentation) and then use these results to do another task like part-of-speech tagging. \r\n",
        "\r\n",
        "There are many NLP pipelines available. \r\n",
        "Small subset of usable pipelines: \r\n",
        "* [SpaCy](https://spacy.io/) - single implementation for each NLP component\r\n",
        "* [Stanza](https://stanfordnlp.github.io/stanza/) -  highly accurate neural network components, can train your own models easily, supports 66 languages\r\n",
        "* [NLTK](https://www.nltk.org/) - Multiple implementations for each NLP component, can build your own pipeline\r\n",
        "* [UDPipe ](https://ufal.mff.cuni.cz/udpipe/1)- Trainable pipeline, language-agnostic\r\n",
        "* [Forte](https://github.com/asyml/forte) - toolkit for building NLP pipelines, decomposes problem into data, models and tasks. More usable for building integrated systems (search documents, analyze, extract documents etc)\r\n",
        "* [TextBlob](https://textblob.readthedocs.io/en/dev/) - extension of NLTK (simplified manner), good for small projects where state-of-the-art results are not needed \r\n",
        "* [CogCompNLP](https://github.com/CogComp/cogcomp-nlp) (Java tool) - developed by the University of Illinois, can process text (locally and remotely), a lot of components. \r\n",
        "\r\n",
        "In this Lab, we will use [Stanza](https://stanfordnlp.github.io/stanza/) pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reJA8mLwTmKe"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo2dN85xThJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0898e28-394c-429c-b6e3-b3f8da004b07"
      },
      "source": [
        "import stanza\r\n",
        "stanza.download('en') # download the appropriate models \r\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 30.2MB/s]                    \n",
            "2021-02-16 08:49:15 INFO: Downloading default packages for language: en (English)...\n",
            "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/en/default.zip: 100%|██████████| 411M/411M [03:34<00:00, 1.91MB/s]\n",
            "2021-02-16 08:52:58 INFO: Finished downloading models and saved to /root/stanza_resources.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xo06Kl-fCk8"
      },
      "source": [
        "\r\n",
        "<img src=\"https://stanfordnlp.github.io/stanza/assets/images/pipeline.png\" >\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMOp-0nwTtKo"
      },
      "source": [
        "Stanza pipeline contains many processors that depend on each other: \r\n",
        "* tokenize processor, \r\n",
        "* multi-word tokens (MWT) processor, \r\n",
        "* POS processor, \r\n",
        "* lemma processor, \r\n",
        "* depparse processor, \r\n",
        "* NER (named entity recognizer) processor,\r\n",
        "* sentiment processor. \r\n",
        "\r\n",
        "Each of these processors have specific requirements. For example depparse processor needs tokenize, MWT, POS and lemma annotations. \r\n",
        "\r\n",
        "We can define the pipeline as follows: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khMwnumiZ61h"
      },
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse, ner, sentiment') #mwt is not available from official model list\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcfkK5sNomh7"
      },
      "source": [
        "raw_text = (\"The brown fox is quick and he is jumping over the lazy dog\" )\r\n",
        "print(raw_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyKOIxRtpQvp"
      },
      "source": [
        "Lets analyse this text with Stanza pipeline: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MecJR-_-pOtt"
      },
      "source": [
        "doc = ...\r\n",
        "print(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14JMm5u3rUzV"
      },
      "source": [
        "After analysis, the pipeline gives you the Document object.\r\n",
        "Document has the following properties: text, sentences, entities, num_tokens, num_words. A Sentence objects inside Document represents a sentence, this object contains a list of Tokens. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weHOyJE8rB5x"
      },
      "source": [
        "print(f\"Text: {...}\")\r\n",
        "print(f\"Dependencies: {...}\")\r\n",
        "print(f\"Tokens: {...}\")\r\n",
        "print(f\"Words: {...}\")\r\n",
        "print(f\"XPOS: {...}\")\r\n",
        "print(f\"Entities: {...}\")\r\n",
        "print(f\"Sentiment: {...}\") # Available for English, Chinese, German"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpBPkT5FrA5m"
      },
      "source": [
        " Finally, a Word object has all the analysis results, that can be accesses with the following attributes: id, text, lemma, xpos, upos, feats (morphological features), head , deprel (dependency relation between this word and its head), and misc .\r\n",
        "\r\n",
        "### Part of Speech \r\n",
        "Parts of speech (POS) are specific lexical categories to which words are assigned based\r\n",
        "on their syntactic context and role. \r\n",
        "The main POS are nouns, verbs, adjectives, and\r\n",
        "adverbs. The process of classifying and labeling POS tags for words is defined as parts of\r\n",
        "speech tagging (POS tagging).\r\n",
        "\r\n",
        "Stanza outputs universal POS (UPOS) and language sepcific POS (XPOS). \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj_feYfKi-UZ"
      },
      "source": [
        "pos_tagged = [...]\r\n",
        "pd.DataFrame(pos_tagged, columns=['Word', 'UPOS', 'XPOS']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WThpymp1lOdR"
      },
      "source": [
        "### Morphological Tagging\r\n",
        "By definition, a morpheme is the smallest unit of\r\n",
        "language that has distinctive meaning. This includes things like\r\n",
        "words, prefixes, suffixes, and so on, which have their own distinct\r\n",
        "meaning. Morphology is the study of the structure and meaning of\r\n",
        "these distinctive units or morphemes in a language. There are specific\r\n",
        "rules and syntaxes that govern the way morphemes can combine. For example, the word unbreakable is composed of three morphemes: \r\n",
        "* *un* - a bound morpheme signifying *not*\r\n",
        "* *break* - the root morpheme \r\n",
        "* *able* - a free morpheme signifying *can be done*\r\n",
        "\r\n",
        "In the UD corpora, these attributes are annotated as feature-value pairs for each\r\n",
        "token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGbfoK9ZlrAu"
      },
      "source": [
        "morph_tagged = [...]\r\n",
        "pd.DataFrame(morph_tagged, columns=['Word', 'Feats'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvV8CbFBlrX9"
      },
      "source": [
        "### Dependency Parsing\r\n",
        "\r\n",
        "Syntax usually envolves the study of sentences, phrases, words, and\r\n",
        "their structures. This includes researching how words are combined\r\n",
        "grammatically to form phrases and sentences. Syntactic order of\r\n",
        "words used in a phrase or a sentence matter since the order can\r\n",
        "change the meaning entirely.\r\n",
        "\r\n",
        "In dependency-based parsing, we try to use dependency-based grammars to analyze\r\n",
        "and infer both structure and semantic dependencies and relationships between tokens\r\n",
        "in a sentence.\r\n",
        "\r\n",
        "The basic principle behind a dependency grammar is that in any sentence in the\r\n",
        "language, all words except one have some relationship or dependency on other words\r\n",
        "in the sentence. The word that has no dependency is called the root of the sentence. The\r\n",
        "verb is taken as the root of the sentence in most cases. All the other words are directly or\r\n",
        "indirectly linked to the root verb using links , which are the dependencies\r\n",
        "\r\n",
        "Stanza outputs the head and deprel, head being the head of the word, which is either value of ID or zero (meaning that the word is the root), and deprel being the relation to the head. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SyhiUt0nds3"
      },
      "source": [
        "synt_tagged = [...]\r\n",
        "pd.DataFrame(synt_tagged, columns=['Word', 'Head', 'Deprel']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84zFSHCgyGxF"
      },
      "source": [
        "We can also visualize the dependency syntax tree using SpaCy: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo0Cde3TyGP1"
      },
      "source": [
        "import spacy\r\n",
        "from spacy import displacy\r\n",
        "!python -m spacy download en_core_web_sm\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWoowxd2yPNX"
      },
      "source": [
        "nlp_spacy =  spacy.load(\"en_core_web_sm\")\r\n",
        "displacy.render(nlp_spacy(raw_text), jupyter=True, options={'distance':100, 'arrow_stroke':1.5, 'arrow_width':8})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkdodVcInd6A"
      },
      "source": [
        "### Named Entity Recognition\r\n",
        "\r\n",
        "A classical problem in information extraction is to recognize and extract mentions of\r\n",
        "named entities in text. In news documents, the core entity types are people, locations, and\r\n",
        "organizations; more recently, the task has been extended to include amounts of money,\r\n",
        "percentages, dates, and times.\r\n",
        "\r\n",
        "Usually BIO notation is used for named entity recognition. Each token at the\r\n",
        "beginning of a name span is labeled with a B- prefix; each token within a name span is labeled with an I- prefix. These prefixes are followed by a tag for the entity type, e.g. B-LOC\r\n",
        "for the beginning of a location. Tokens\r\n",
        "that are not parts of name spans are labeled as O.\r\n",
        "\r\n",
        "Stanza uses BIOES representation, where E denotes ending and S denotes single element. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-4ijn14ntDW"
      },
      "source": [
        "ner_text = \"The U.S. Army captured Atlanta on May 14, 1864.\"\r\n",
        "ner_doc = ...\r\n",
        "ner_tagged = [...]\r\n",
        "pd.DataFrame(ner_tagged, columns=['Word', 'NER']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgxXCoJ8pZBL"
      },
      "source": [
        "Now we can put the whole output in [ConNLL-U format](https://universaldependencies.org/format.html): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FwlK4URxho2"
      },
      "source": [
        "columns = ['ID', 'FORM', 'LEMMA', 'UPOS', 'XPOS', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']\r\n",
        "tagged = [...]\r\n",
        "pd.DataFrame(tagged, columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJJNzsuHBHyA"
      },
      "source": [
        "### Performance \r\n",
        "\r\n",
        "You can check the performance for other treebanks [here](https://stanfordnlp.github.io/stanza/performance.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "HO_dnlw6BOH8",
        "outputId": "57722eed-41de-42d7-8f16-956d1694fe82"
      },
      "source": [
        "columns = ['Treebank','Tokens', 'Sentences', 'UPOS', 'XPOS', 'Feats', 'UAS', 'LAS', 'LEMMAS']\r\n",
        "results = [('UD_English-EWT', 99.01,\t81.13, 95.4,\t95.12, 96.11, 86.22,\t83.59,\t97.21),\r\n",
        "           ('UD_Estonian-EDT', 99.96, 93.32, 97.19,98.04, 95.77,86.68, 83.82, 96.05 ), \r\n",
        "           ('UD_Russian-SynTagRus',99.57,\t98.86, 98.2,\t99.57,\t95.91, \t92.38,\t90.6 ,97.51)]\r\n",
        "pd.DataFrame(results, columns=columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Treebank</th>\n",
              "      <th>Tokens</th>\n",
              "      <th>Sentences</th>\n",
              "      <th>UPOS</th>\n",
              "      <th>XPOS</th>\n",
              "      <th>Feats</th>\n",
              "      <th>UAS</th>\n",
              "      <th>LAS</th>\n",
              "      <th>LEMMAS</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>UD_English-EWT</td>\n",
              "      <td>99.01</td>\n",
              "      <td>81.13</td>\n",
              "      <td>95.40</td>\n",
              "      <td>95.12</td>\n",
              "      <td>96.11</td>\n",
              "      <td>86.22</td>\n",
              "      <td>83.59</td>\n",
              "      <td>97.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>UD_Estonian-EDT</td>\n",
              "      <td>99.96</td>\n",
              "      <td>93.32</td>\n",
              "      <td>97.19</td>\n",
              "      <td>98.04</td>\n",
              "      <td>95.77</td>\n",
              "      <td>86.68</td>\n",
              "      <td>83.82</td>\n",
              "      <td>96.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>UD_Russian-SynTagRus</td>\n",
              "      <td>99.57</td>\n",
              "      <td>98.86</td>\n",
              "      <td>98.20</td>\n",
              "      <td>99.57</td>\n",
              "      <td>95.91</td>\n",
              "      <td>92.38</td>\n",
              "      <td>90.60</td>\n",
              "      <td>97.51</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Treebank  Tokens  Sentences   UPOS  ...  Feats    UAS    LAS  LEMMAS\n",
              "0        UD_English-EWT   99.01      81.13  95.40  ...  96.11  86.22  83.59   97.21\n",
              "1       UD_Estonian-EDT   99.96      93.32  97.19  ...  95.77  86.68  83.82   96.05\n",
              "2  UD_Russian-SynTagRus   99.57      98.86  98.20  ...  95.91  92.38  90.60   97.51\n",
              "\n",
              "[3 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRrwrbvmrhGP"
      },
      "source": [
        "What can we now do with all of this information? \r\n",
        "\r\n",
        "1. Input to another task in NLP like summarization, information extraction, machine translation etc. \r\n",
        "2. Extract some phrases, sentences (for example, extract NOUN-VERB pairs for further analysis like clustering) \r\n",
        "3. any more ideas? \r\n",
        "\r\n",
        "Why it might be a bad idea to use pretrained pipelines? \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0NHpBwoAA9P"
      },
      "source": [
        "### Analysing movie \"The Room\"\r\n",
        "Let's analyse one of the greatest movies of all time \"The Room\" by Tommy Wiseau.\r\n",
        "\r\n",
        "We know that the main character Johnny shot himself dead in the end. The question is can we see that his mood changes from his text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZnKELA1AEiD"
      },
      "source": [
        "johnny_lines = []\r\n",
        "with  open('the_room.txt', 'r', encoding='utf-8') as f: \r\n",
        "  for line in f: \r\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LBe1Q65ASLV"
      },
      "source": [
        "johnny_lines[0], johnny_lines[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sShDBiXJAP0K"
      },
      "source": [
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "analysed_lines = []\r\n",
        "for johnny_line in tqdm(johnny_lines): \r\n",
        "  doc = ...\r\n",
        "  ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_50J0ApGAbdK"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1olhPlVAkc4"
      },
      "source": [
        "Did it work and why? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on20trBaAqPv"
      },
      "source": [
        "... # alternate approach"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}